# -*- coding: utf-8 -*-
"""product_filter_from_blogs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15fX9JnrrwaFuiQcgkNkm3AClWgilDm0Y
"""

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/google_scraped_images_data.csv")
data_csv = df.get("alt")
data_csv.head()

data_csv[:30]
len(data_csv)

import pandas as pd
import re

# Load the CSV file containing the "alt" column.
csv_name = "/content/drive/MyDrive/google_scraped_images_data.csv"
df = pd.read_csv(csv_name)

def clean_title(title):
    # Remove any occurrences of "amazon.com" (case-insensitive)
    title = re.sub(r'amazon\.com', '', title, flags=re.IGNORECASE)
    # Remove Chinese characters (unicode range for CJK Unified Ideographs)
    title = re.sub(r'[\u4e00-\u9fff]', '', title)
    # Optionally, remove extra whitespace
    title = ' '.join(title.split())
    return title

# Check that the "alt" column exists.
if 'alt' not in df.columns:
    raise ValueError("CSV file must contain an 'alt' column.")

# Define a simple heuristic to assign a label:
# We'll assume:
# - 1: Real product name.
# - 0: Blog title or non-product.
#
# For example, if the title contains "review", "guide", or "best" (case-insensitive),
# we might consider it a blog or editorial title. Adjust the logic as needed.
def assign_label(title):
    title_lower = title.lower()
    # These keywords may indicate a blog or review article.
    blog_keywords = ["review", "guide", "best","icon","cant","vector","top","level","Innovation","Masterclass","brand","campaigns","INMA","Planner","Charyl","Quotes","Wikipedia","Mean","Guest","guest","Example","Keywords","Amazing","Examples","Idea","Ideas","Types","Type","Inspire","Popular","Types","Guest","Benefits","How","Free","Definition","Blog","Blogger","blogging","Definition","What","personal", "top","impact","impressions","solving","how","not","issue","buy","lowest price","price","new","after","day","cheap","affordable"]
    for keyword in blog_keywords:
      keyword=keyword.lower()
      if re.search(r'\b' + re.escape(keyword) + r'\b', title_lower):
          return 0  # Blog or review article
    # Otherwise, assume it's a product name.
    return 1
df.dropna(inplace=True)
# Convert the DataFrame to a list of training examples.
# Each training example is a dictionary with "title" and "label".
training_data = [{"title": clean_title(row["alt"]), "label": assign_label(str(row["alt"]))} for idx, row in df.iterrows()]

# Print the converted training data
for example in training_data:
    print(example)

len(training_data)

data = training_data[:70000]
len(data)

import random
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
import joblib

# -------------------------------
# Simulated Dataset
# -------------------------------
# Each item is a dictionary with a 'title' and a 'label'
# Label: 1 = real product name, 0 = blog title (or non-product)
 # Assuming training_data is already defined

# Extract titles and true labels.
titles = [d["title"] for d in data]
true_labels = np.array([d["label"] for d in data])

# -------------------------------
# Feature Extraction: TF-IDF
# -------------------------------
# We convert titles to a numeric feature space.
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(titles)

# -------------------------------
# Online Learning Classifier Setup
# -------------------------------
# Using SGDClassifier with a logistic loss (log_loss) for logistic regression.
clf = SGDClassifier(loss='log_loss', random_state=42)
# Initialize the classifier with our dataset's classes (0 and 1).
clf.partial_fit(X, true_labels, classes=np.array([0, 1]))

# -------------------------------
# Contextual Bandit Setup (Epsilon-Greedy)
# -------------------------------
epsilon = 0.2  # Exploration probability
total_reward = 0

# We'll simulate multiple passes (epochs) over the data to let the classifier improve.
num_epochs = 10

print("Starting contextual bandit training...\n")
for epoch in range(num_epochs):
    total_reward=0
    print(f"Epoch {epoch+1}")
    # Shuffle the data order each epoch.
    indices = list(range(len(titles)))
    random.shuffle(indices)
    for i in indices:
        title = titles[i]
        true_label = true_labels[i]
        # Transform the title into the feature vector.
        x = vectorizer.transform([title])

        # Epsilon-greedy: with probability epsilon, choose a random action (0 or 1)
        if random.random() < epsilon:
            action = random.choice([0, 1])
        else:
            # Otherwise, use the classifier's current prediction.
            action = clf.predict(x)[0]

        # Define reward: 1 if prediction matches true label, else 0.
        reward = 1 if action == true_label else 0
        total_reward += reward

        # Update the classifier with the true label (simulate receiving feedback).
        clf.partial_fit(x, [true_label])

    print(f"Total reward after epoch {epoch+1}: {total_reward}\n")

print("Training completed.")

# -------------------------------
# Store the Vectorizer and Classifier
# -------------------------------
# Save the vectorizer to a file (e.g., "tfidf_vectorizer.pkl")
joblib.dump(vectorizer, "/content/drive/MyDrive/my_vectorizer.pkl")
# Optionally, save the classifier as well:
joblib.dump(clf, "/content/drive/MyDrive/my_classifier.pkl")

print("Vectorizer and classifier have been stored successfully.")

train_data = training_data[:70000]
test_data = training_data[70000:]

# Extract titles and labels for training and testing.
train_titles = [d["title"] for d in train_data]
train_labels = np.array([d["label"] for d in train_data])
test_titles = [d["title"] for d in test_data]
test_labels = np.array([d["label"] for d in test_data])

from sklearn.metrics import accuracy_score, classification_report

# -------------------------------
# Evaluate on the Test Set
# -------------------------------
X_test = vectorizer.transform(test_titles)
predictions = clf.predict(X_test)
acc = accuracy_score(test_labels, predictions)
report = classification_report(test_labels, predictions, target_names=["Blog", "Product"])

print(f"\nTest Accuracy: {acc*100:.2f}%")
print("\nClassification Report:")
print(report)

import pandas as pd
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier

# -------------------------------
# Load Trained Model and Vectorizer
# -------------------------------
vectorizer = joblib.load("/content/drive/MyDrive/my_vectorizer.pkl")  # Load the trained TF-IDF vectorizer
clf = joblib.load("/content/drive/MyDrive/my_classifier.pkl")  # Load the trained classifier

# -------------------------------
# Load Dataset for Inference
# -------------------------------
csv_path = "/content/drive/MyDrive/google_scraped_images_data.csv"  # Update the path if necessary
df = pd.read_csv(csv_path)

if "alt" not in df.columns:
    raise ValueError("CSV file must have a 'alt' column.")

# -------------------------------
# Run Inference on Titles
# -------------------------------
titles = df["alt"].astype(str).tolist()
X_new = vectorizer.transform(titles)  # Transform titles using the trained vectorizer
predictions = clf.predict(X_new)  # Predict using the classifier

# Get confidence scores (optional)
if hasattr(clf, "predict_proba"):
    probabilities = clf.predict_proba(X_new)[:, 1]  # Probability of being a product
else:
    probabilities = clf.decision_function(X_new)

# -------------------------------
# Store Predictions in CSV
# -------------------------------
df["prediction"] = predictions  # 1 = Product, 0 = Blog
df["confidence"] = probabilities  # Store confidence scores

# Save the updated dataset
output_path = "/content/drive/MyDrive/your_images_classified.csv"
df.to_csv(output_path, index=False)

print(f"Inference complete. Results saved to {output_path}")

